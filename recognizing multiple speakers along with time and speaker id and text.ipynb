{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recognizing multiple speakers ID and time \n",
    "##### This code is designed to recognize multiple speakers and extract their speech information, including speaker IDs, timestamps, and the actual text spoken. It primarily utilizes the PyTorch framework, along with speech recognition libraries like PyAnnote.audio, audio processing libraries like PyDub and AudioSegment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setting up the necessary libraries and tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchvision is not available - cannot save figures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker SPEAKER_01 from 1.3s to 18.4s: ఒక రోజు ఒక అడవిలో తాబేలు నెమ్మదిగా వెళుతూ ఉంటే ఒక కుందేలు తన దగ్గరికి వచ్చి తనను వెక్కిరిస్తుంది నువ్వు ఇంత మెల్లగా వెళ్తున్నావ్ ఏంటి అప్పుడు వాళ్ళిద్దరూ ఒక పోటీ పెట్టుకుంటారు ఎవరు ముందుగా ఒక ఆ ప్రదేశాన్ని చేరుకుంటారు\n",
      "Speaker SPEAKER_02 from 19.5s to 41.0s: వెంటనే తాబేలు కళ్ళు తన లోపలికి కదలకుండా ఉండిపోయింది నాకు తాబేలు దగ్గరికి వెళ్లి దాన్ని పట్టుకొని చూసింది పైన తప్ప గట్టిగా తగలలేదు తాబేలు తిరిగేసి మూతి దగ్గరికి పెట్టుకుని ఇలా నక్క తనను పరీక్షిస్తూ ఎంతసేపు తాబేలు ప్రాణాలు అరచేతిలో పెట్టుకుని ఊపిరి పట్టుకొని ఉన్నది\n",
      "Speaker SPEAKER_00 from 41.4s to 59.5s: ఒక అడవిలోని చెరువులో ఒక తాబేలు ఉండేది ఒక రోజు సాయంత్రం అది నీటి నుంచి బయటికి వచ్చి ఒడ్డున చేరుకుంది ఇంతలో అక్కడికి ఒక నక్క వచ్చింది దాన్ని చూసి నీటిలోకి వెళ్లిపోవాల్సి వచ్చింది తాబేలు కానీ ఇంతలో నక్క దాన్ని చూసింది వెంటనే తాబేలు కళ్ళు తల లోపలికి లో\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pydub import AudioSegment\n",
    "import speech_recognition as sr\n",
    "from pyannote.audio import Pipeline\n",
    "import torch\n",
    "\n",
    "def convert_to_wav(audio_file_path):\n",
    "    audio = AudioSegment.from_file(audio_file_path)\n",
    "    wav_file_path = os.path.splitext(audio_file_path)[0] + \".wav\"\n",
    "    audio.export(wav_file_path, format=\"wav\")\n",
    "    return wav_file_path\n",
    "\n",
    "def recognize_speech_from_audio_segment(segment, language='te-IN'):\n",
    "    recognizer = sr.Recognizer()\n",
    "    wav_file_path = \"temp_segment.wav\"\n",
    "    segment.export(wav_file_path, format=\"wav\")\n",
    "\n",
    "    try:\n",
    "        with sr.AudioFile(wav_file_path) as source:\n",
    "            audio = recognizer.record(source)\n",
    "        \n",
    "        text = recognizer.recognize_google(audio, language=language)\n",
    "        return text\n",
    "    except sr.UnknownValueError:\n",
    "        return \"Could not understand audio.\"\n",
    "    except sr.RequestError as e:\n",
    "        return f\"Could not request results; {e}\"\n",
    "    finally:\n",
    "        if os.path.exists(wav_file_path):\n",
    "            os.remove(wav_file_path)\n",
    "\n",
    "def voice_typing_from_file(audio_file_path):\n",
    "    wav_file_path = convert_to_wav(audio_file_path)\n",
    "    \n",
    "    # Initialize the speaker diarization pipeline\n",
    "    pipeline = Pipeline.from_pretrained(\n",
    "        \"pyannote/speaker-diarization-3.1\",\n",
    "        use_auth_token=\"your_huggingface_auth_token\"\n",
    "    )\n",
    "\n",
    "    # Send the pipeline to GPU (if available)\n",
    "    if torch.cuda.is_available():\n",
    "        pipeline.to(torch.device(\"cuda\"))\n",
    "\n",
    "    # Run the diarization pipeline\n",
    "    diarization = pipeline(wav_file_path)\n",
    "    \n",
    "    # Process each speaker segment\n",
    "    audio = AudioSegment.from_wav(wav_file_path)\n",
    "    results = []\n",
    "\n",
    "    current_speaker = None\n",
    "    current_start_time = None\n",
    "    current_text = []\n",
    "\n",
    "    for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "        start_time = turn.start * 1000  # pydub works in milliseconds\n",
    "        end_time = turn.end * 1000\n",
    "        segment = audio[start_time:end_time]\n",
    "        \n",
    "        recognized_text = recognize_speech_from_audio_segment(segment)\n",
    "\n",
    "        if speaker == current_speaker:\n",
    "            current_text.append(recognized_text)\n",
    "            current_end_time = end_time\n",
    "        else:\n",
    "            if current_speaker is not None:\n",
    "                results.append((current_speaker, current_start_time, current_end_time, ' '.join(current_text)))\n",
    "            current_speaker = speaker\n",
    "            current_start_time = start_time\n",
    "            current_end_time = end_time\n",
    "            current_text = [recognized_text]\n",
    "    \n",
    "    if current_speaker is not None:\n",
    "        results.append((current_speaker, current_start_time, current_end_time, ' '.join(current_text)))\n",
    "\n",
    "    for speaker, start_time, end_time, text in results:\n",
    "        print(f\"Speaker {speaker} from {start_time / 1000:.1f}s to {end_time / 1000:.1f}s: {text}\")\n",
    "\n",
    "    # Clean up the temporary WAV file\n",
    "    if os.path.exists(wav_file_path):\n",
    "        os.remove(wav_file_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Specify the path to the audio file you want to process\n",
    "    audio_file_path =r\"D:\\Inputs & Outputs\\sample-3.MOV\"\n",
    "    voice_typing_from_file(audio_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
